\chapter{Modeling}
\thispagestyle{fancy}
\label{chap:modeling}





\section{Introduction}

Modeling is the process of constructing abstract representations of real-world systems, behaviors, or phenomena. These representations—whether mathematical, statistical, or computational—enable analysis, simulation, prediction, and understanding of complex structures and dynamics.

This chapter will introduces several types of models used across disciplines, with a focus on those that capture structure, uncertainty, and sequential behavior. Topics include deterministic models, probabilistic frameworks, and data-driven models. The goal is to present models not only as tools for approximation or prediction, but as frameworks for organizing knowledge and reasoning about systems.










\section{Markov Models}

\subsection{Overview}

In a previous project I was working on for Dungeons \& Dragons, I created a model for generating random names and character sequences. That project code can be found here: 
\begin{center}
\url{https://github.com/torodean/DnD/blob/main/templates/creator.py}. 
\end{center}
The functionality was based on transition patterns between characters of preexisting names. I only later found out that this was referred to as a Markov model. The features related to this will be discussed here in a more generalized form. These models can be used to create elements which follow a similar pattern of an input sequence (such as creating predictive text).

\subsection{First-Order Markov Model}

This section contains an explanation of how the markov model functions. Consider some set of sequences $S$ for some arbitrary type, where each element is denoted by a capitalized letter. For example, let 
\begin{align}
	S=\{ABC, ABD, BAD\} \label{example-first-order-markov-sequence-small}
\end{align}. 
The probability matrix $P$ is constructed by determining all of the elements which follow another, and at what probability that element has of following the others (The probabilities of elements following some element $K$ is $P_K$). that is $P(S) = \{K:P_K \forall K \in S\}$.

The set of elements which exist in $S$ are ${A, B, C, D, \emptyset}$, where $\emptyset$ denotes the absence of an element (or beginning/end of a sequence). Starting with $A$, we can see that the $A$ element is followed only by $B$ (twice), and $D$ (once) in $S$. The total number of elements ever following an $A$ is thus three. The probabilities following an element $A$ is thus
\begin{align}
P_A = \begin{cases}
  B & :\text{twice} \\
  D & :\text{once}
\end{cases} \implies P_A = \begin{cases}
  B & : 66.\overline{6}\% \\
  D & : 33.\overline{3}\%
\end{cases} = \{B:0.\overline{6}, D:0.\overline{3}\}
\end{align}
Following this same process for the other elements gives
\begin{align}
P_B &= \{A:0.\overline{3}, C:0.\overline{3}, D:0.\overline{3}\} \\ P_C &= P_D = \{\emptyset:1.0\}\\ P_\emptyset &= \{A:0.\overline{6}, B:0.\overline{3}\}.
\end{align}
The total probability matrix for this set of sequences would then be
\begin{align}
P(S) = \{A:P_A, B:P_B, C:P_C, D:P_D, \emptyset: P_\emptyset\} = \begin{cases}
A:\{B:0.\overline{6}, D:0.\overline{3}\} \\
B: \{A:0.\overline{3}, C:0.\overline{3}, D:0.\overline{3}\} \\
C: \{\emptyset:1.0\} \\
D: \{\emptyset:1.0\} \\
\emptyset: \{A:0.\overline{6}, B:0.\overline{3}\}.
\end{cases}
\end{align}
The $\emptyset$ is a special case in that it represents the first character of a sequence (there is never a character after the last). This format may not look like a matrix at all, but it can be re-written to matrix format. First, note that there are a total of 5 elements ($A$, $B$, $C$, $D$, $\emptyset$) which will give a $5 \times 5$ matrix for all possible combinations. The matrix is configured such that both the rows and columns span from $A\rightarrow\emptyset$, covering all the elements of the set. The matrix value of $a, b$ then represents the probability that element $a$ will be proceeded by element $b$.
\begin{align}
P(S) = \left[
\begin{matrix}
0 & 0.\overline{3} & 0 & 0 & 0.\overline{6} \\ 
0.\overline{6} & 0 & 0 & 0 & 0.\overline{3} \\ 
0 & 0.\overline{3} & 0 & 0 & 0 \\ 
0.\overline{3} & 0.\overline{3} & 0 & 0 & 0\\ 
0 & 0 & 1.0 & 1.0 & 0 \\ 
\end{matrix}\right] \label{eqn:Example-prob-matrix-1}
\end{align}
This probability matrix thus represents the probability of an element proceeding another in one of the given sequences. Each column of the matrix should total to $1.0$, as they represent the total set of elements proceeding another. It can be used to generate new sequences which adhere to similar patterns of the input sequences. With larger data sets, more possibilities of sequences typically arise as probable outputs. 

One important feature of these models is that under low-entropy (the model is derived from a deterministic source), a uniquely resolvable input set (You can reconstruct exactly one input set) and with enough metadata (initial state, model size, model order, etc), the model can be used to reconstruct the original data.







\subsection{A Larger Example}

Consider the following set of sequences:
\begin{align}
	S=\{ABCDE, ABDEE, AABCD, ACDCE, AABCD\}
\end{align}
The set of all elements of this sequence are $\{A, B, C, D, E, \emptyset\}$. There are a total of 6 elements. This set of sequences has 7 $A$'s, 4 $B$'s, 5 $C$'s, 5 $D$'s, 4 $E$'s, and 5 $\emptyset$'s (beginning of each sequence). The probabilities $P_k$ are then
\begin{align}
	P_A &= \{A:2/7, B:4/7, C:1/7, D:0, E:0, \emptyset:0 \} \\
	P_B &= \{A:0, B:0, C:3/4, D:1/4, E:0, \emptyset:0 \} \\
	P_C &= \{A:0, B:0, C:0, D:4/5, E:1/5, \emptyset:0 \} \\
	P_D &= \{A:0, B:0, C:1/5, D:0, E:2/5, \emptyset:2/5 \} \\
	P_E &= \{A:0, B:0, C:0, D:0, E:1/4, \emptyset:3/4 \} \\
	P_\emptyset &= \{A:5/5, B:0, C:0, D:0, E:0, \emptyset: 0\} 
\end{align}
This gives a probability matrix $P(S)$ of
\begin{align}
	P(S) = \left[\begin{matrix}
		2/7 & 0 & 0 & 0 & 0 & 5/5 \\
		4/7 & 0 & 0 & 0 & 0 &  0 \\
		1/7 & 3/4 & 0 & 1/5 & 0 & 0 \\
		0 & 1/4 & 4/5 & 0 & 0 & 0 \\
		0 & 0 & 1/5 & 2/5 & 1/4 & 0 \\
		0 & 0 & 0 & 2/5 & 3/4 & 0 \\
	\end{matrix}\right]
\end{align}










\subsection{Predicting Input Size}

A probability matrix for a markov model is generated with a set of sequences. Each sequence $s$ has a size, and the set of all sequences $S$ also has a size. If the size of each sequence $s$ is fixed, there will always be a minimum possible size of the $S$ needed to generate the probability matrix. This analysis will being by using a fixed size for all $s\in S$.

Given an input probability matrix $P$, a limited prediction of the input data
size can be made. Suppose $P(S)$ forms an $n \times n$ matrix, implying that it contains $n$ elements. The matrix is formed by some number of combinations of these $n$ elements. For visualization, suppose the matrix looks something like the following:
\begin{align}
P(S) = \left[
\begin{matrix}
s_{00} & s_{10} & s_{20} & \cdots & s_{n0} \\
s_{01} & s_{11} & s_{21} & \cdots & s_{n1} \\
s_{02} & s_{12} & s_{22} & \cdots & s_{n2} \\
\vdots & \vdots & \vdots & \ddots & \vdots \\
s_{0n} & s_{1n} & s_{2n} & \cdots & s_{nn} \\
\end{matrix}\right]
\end{align}
As mentioned before, the probabilities of each column $k$ (each column represents a different element of $S$, so $k$ here, is just any element $k \in S$) must add to 1, giving
\begin{align}
P(k) = \sum_{i=0}^{n}s_{in} = 1, \forall k \in S.
\end{align}

Every element $k\in S$ will appear an integer number of times (since an element either appears or does not). This means, that every probability $s$ will be a rational value, $s\in\mathbb{Q}$ (since it is defined as the total occurrences of $k$ over the total possible occurrences of any element). Therefore, we can define $s^*_{ik}\in \mathbb{N}$ such that $s^*_{ik} = s_{ik}N_k$, where $N_k\in\mathbb{N}$. Given that $N_k$ is the least common denominator (LCD) when summing all values of $s_{ik}$ to 1, this value of $N_k$ would then be the minimum possible number of total elements of $k$, denoted $T_k$, in the initial sequence which produced this matrix. This gives
\begin{align}
\min (T_k) = \sum_{i=0}^{n}s^*_{ik} = N_k, \forall k \in S.
\end{align}
Note that the $\emptyset$ element is a special case here. It is defined as the start or end of a sequence and therefore there will always be $n$ of them from the reference of following an element, and $n$ from the reference of preceding an element.

This is more of an observational conclusion than a formal theorem. Notice in equation \ref{eqn:Example-prob-matrix-1}, for the first column, we have $\frac{2}{3}$ and $\frac{1}{3}$ as the probabilities. When summed to 1, the LCD of these is 3. This correctly matches the value of the number of $A$ elements (which corresponds to the first column) in the corresponding sequence of $S$ which was used. However, if the initial sequence had 6 elements, and the probabilities were thus $\frac{4}{6} = \frac{2}{3}$ and $\frac{2}{6} = \frac{1}{3}$ (respectively), then the value would have been incorrect because the fractions may have been reduced. This is the reason I say it is $\min (T_k)$ rather than $T_k$. Given that the fractions for the probabilities are left in an non-reduced form, or are irreducible to begin with, this would be $T_k$. But for general purposes, the probabilities are likely always reduced to simplest form and thus we can only determine a minimum value of $T_k$. It will be true that the total number is some multiple of $T_k$, however, which can be valuable information.

There may be some interesting insights to gain from the rows of the probability matrix as well. Given the existence of $N_k$, which has already been established, the sum of the row $R$ for some element $k$ can be written as
\begin{align}
	R(k) = \sum_{i=0}^{n} s_{ki} = \sum_{i=0}^{n} \frac{s^*_{ki}}{N_i} = \frac{s^*_{k0}}{N_0} + \frac{s^*_{k1}}{N_1}\frac{s^*_{k2}}{N_2} + \cdots + \frac{s^*_{kn}}{N_n}.
\end{align}
If $N_i$ is an unreduced LCD for the values of its row, then these coefficients give the total number of elements $k$ in the set of sequences
\begin{align}
	T_k = \sum_{i=0}^{n} s^*_{ki}. 
\end{align}
Again, this is more of an observational conclusion than a formal theorem. But this can be seen in the following way. Each element, when appearing in a sequence, will add some minimum amount of probability $p_{min}$ to a corresponding position in the matrix. this amount will always be equal to $1/T_k$. If the elements appear after another element more than once, say, $s^*_{ki}$ times, then this coefficient increments, giving $s^*_{ki}/T_k$. For unreduced fractional forms of each probability, $T_k = N_k$, so $s^*_{ki}/N_k$ is just a count of how many elements there are. Thus, by summing these counters for each case (an element proceeding another element), we have the total number of elements in a set of sequences.



\unfinished


























\subsection{Higher-Order Markov Model}

A higher order Markov model follows similar principals as a first-order Markov model. The difference is how many elements in sequence are tracked and considered when determining another. Consider the same sequence found in \ref{example-first-order-markov-sequence-small}.
Suppose the order of this model is $O$ (that is a $O$-order Markov model). This would mean that instead of calculating the probability that an element follows another, the probability matrix would instead contain the probability of an element following any sequence of elements from 1 to $O$. For example, suppose we want $O=2$, giving a second order Markov model. For the sequence in \ref{example-first-order-markov-sequence-small}, The possible combinations of any sequence of 1 or 2 elements are the same elements in the first-order Markov model with the union of the additional probabilities of every sequence of 2 elements. The sequence of two elements within $S$ in this case are $AB$ (twice), $BC$ (once), $BD$ (once), $BA$ (once), and $AD$ (once). The probability for a second order is denoted by $P^2$. 



