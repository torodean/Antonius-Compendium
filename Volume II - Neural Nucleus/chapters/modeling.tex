\chapter{Modeling}
\thispagestyle{fancy}
\label{chap:modeling}





\section{Introduction}

Modeling is the process of constructing abstract representations of real-world systems, behaviors, or phenomena. These representations—whether mathematical, statistical, or computational—enable analysis, simulation, prediction, and understanding of complex structures and dynamics.

This chapter will introduces several types of models used across disciplines, with a focus on those that capture structure, uncertainty, and sequential behavior. Topics include deterministic models, probabilistic frameworks, and data-driven models. The goal is to present models not only as tools for approximation or prediction, but as frameworks for organizing knowledge and reasoning about systems.










\section{Markov Models}

\subsection{Overview}

In a previous project I was working on for Dungeons \& Dragons, I created a model for generating random names and character sequences. That project code can be found here: 
\begin{center}
\url{https://github.com/torodean/DnD/blob/main/templates/creator.py}. 
\end{center}
The functionality was based on transition patterns between characters of preexisting names. I only later found out that this was referred to as a Markov model. The features related to this will be discussed here in a more generalized form. These models can be used to create elements which follow a similar pattern of an input sequence (such as creating predictive text).

\subsection{First-Order Markov Model}

This section contains an explanation of how the markov model functions. Consider some set of sequences $S$ for some arbitrary type, where each element is denoted by a capitalized letter. For example, let 
\begin{align}
	S=\{ABC, ABD, BAD\} \label{example-first-order-markov-sequence-small}
\end{align}. 
The probability matrix $P$ is constructed by determining all of the elements which follow another, and at what probability that element has of following the others (The probabilities of elements following some element $K$ is $P_K$). that is $P(S) = \{K:P_K \forall K \in S\}$.

The set of elements which exist in $S$ are ${A, B, C, D, \emptyset}$, where $\emptyset$ denotes the absence of an element (or beginning/end of a sequence). Starting with $A$, we can see that the $A$ element is followed only by $B$ (twice), and $D$ (once) in $S$. The total number of elements ever following an $A$ is thus three. The probabilities following an element $A$ is thus
\begin{align}
P_A = \begin{cases}
  B & :\text{twice} \\
  D & :\text{once}
\end{cases} \implies P_A = \begin{cases}
  B & : 66.\overline{6}\% \\
  D & : 33.\overline{3}\%
\end{cases} = \{B:0.\overline{6}, D:0.\overline{3}\}
\end{align}
Following this same process for the other elements gives
\begin{align}
P_B &= \{A:0.\overline{3}, C:0.\overline{3}, D:0.\overline{3}\} \\ 
P_C &= P_D = \{\emptyset:1.0\}\\ 
P_\emptyset &= \{A:0.\overline{6}, B:0.\overline{3}\}.  
\end{align}
The total probability matrix for this set of sequences would then be
\begin{align}
P(S) = \{A:P_A, B:P_B, C:P_C, D:P_D, \emptyset: P_\emptyset\} = \begin{cases}
A:\{B:0.\overline{6}, D:0.\overline{3}\} \\
B: \{A:0.\overline{3}, C:0.\overline{3}, D:0.\overline{3}\} \\
C: \{\emptyset:1.0\} \\
D: \{\emptyset:1.0\} \\
\emptyset: \{A:0.\overline{6}, B:0.\overline{3}\}.
\end{cases}
\end{align}
The $\emptyset$ is a special case in that it represents the first character of a sequence (there is never a character after the last). This format may not look like a matrix at all, but it can be re-written to matrix format. First, note that there are a total of 5 elements ($A$, $B$, $C$, $D$, $\emptyset$) which will give a $5 \times 5$ matrix for all possible combinations. The matrix is configured such that both the rows and columns span from $A\rightarrow\emptyset$, covering all the elements of the set. The matrix value of $a, b$ then represents the probability that element $a$ will be proceeded by element $b$.
\begin{align}
P(S) = \left[
\begin{matrix}
0 & 0.\overline{3} & 0 & 0 & 0.\overline{6} \\ 
0.\overline{6} & 0 & 0 & 0 & 0.\overline{3} \\ 
0 & 0.\overline{3} & 0 & 0 & 0 \\ 
0.\overline{3} & 0.\overline{3} & 0 & 0 & 0\\ 
0 & 0 & 1.0 & 1.0 & 0 \\ 
\end{matrix}\right] \label{eqn:Example-prob-matrix-1}
\end{align}
This probability matrix thus represents the probability of an element proceeding another in one of the given sequences. Each column of the matrix should total to $1.0$, as they represent the total set of elements proceeding another. It can be used to generate new sequences which adhere to similar patterns of the input sequences. With larger data sets, more possibilities of sequences typically arise as probable outputs. 

One important feature of these models is that under low-entropy (the model is derived from a deterministic source), a uniquely resolvable input set (You can reconstruct exactly one input set) and with enough metadata (initial state, model size, model order, etc), the model can be used to reconstruct the original data.







\subsection{A Larger Example}

Consider the following set of sequences:
\begin{align}
	S=\{ABCDE, ABDEE, AABCD, ACDCE, AABCD\} \label{markov-model-larger-example-sequences}
\end{align}
The set of all elements of this sequence are $\{A, B, C, D, E, \emptyset\}$. There are a total of 6 elements. This set of sequences has 7 $A$'s, 4 $B$'s, 5 $C$'s, 5 $D$'s, 4 $E$'s, and 5 $\emptyset$'s (beginning of each sequence). The probabilities $P_k$ are then
\begin{align}
	P_A &= \{A:2/7, B:4/7, C:1/7, D:0, E:0, \emptyset:0 \} \\
	P_B &= \{A:0, B:0, C:3/4, D:1/4, E:0, \emptyset:0 \} \\
	P_C &= \{A:0, B:0, C:0, D:4/5, E:1/5, \emptyset:0 \} \\
	P_D &= \{A:0, B:0, C:1/5, D:0, E:2/5, \emptyset:2/5 \} \\
	P_E &= \{A:0, B:0, C:0, D:0, E:1/4, \emptyset:3/4 \} \\
	P_\emptyset &= \{A:5/5, B:0, C:0, D:0, E:0, \emptyset: 0\} 
\end{align}
This gives a probability matrix $P(S)$ of
\begin{align}
	P(S) = \left[\begin{matrix}
		2/7 & 0 & 0 & 0 & 0 & 5/5 \\
		4/7 & 0 & 0 & 0 & 0 &  0 \\
		1/7 & 3/4 & 0 & 1/5 & 0 & 0 \\
		0 & 1/4 & 4/5 & 0 & 0 & 0 \\
		0 & 0 & 1/5 & 2/5 & 1/4 & 0 \\
		0 & 0 & 0 & 2/5 & 3/4 & 0 \\
	\end{matrix}\right] \label{markov-model-larger-example-probability-matrix}
\end{align}










\subsection{Predicting Input Size}

A probability matrix for a markov model is generated with a set of sequences. Each sequence $s$ has a size, and the set of all sequences $S$ also has a size. If the size of each sequence $s$ is fixed, there will always be a minimum possible size of the $S$ needed to generate the probability matrix. This analysis will being by using a fixed size for all $s\in S$.

Given an input probability matrix $P$, a limited prediction of the input data
size can be made. Suppose $P(S)$ forms an $n \times n$ matrix, implying that it contains $n$ elements. The matrix is formed by some number of combinations of these $n$ elements. For visualization, suppose the matrix looks something like the following:
\begin{align}
P(S) = \left[
\begin{matrix}
s_{00} & s_{10} & s_{20} & \cdots & s_{n0} \\
s_{01} & s_{11} & s_{21} & \cdots & s_{n1} \\
s_{02} & s_{12} & s_{22} & \cdots & s_{n2} \\
\vdots & \vdots & \vdots & \ddots & \vdots \\
s_{0n} & s_{1n} & s_{2n} & \cdots & s_{nn} \\
\end{matrix}\right]
\end{align}
As mentioned before, the probabilities of each column $k$ (each column represents a different element of $S$, so $k$ here, is just any element $k \in S$) must add to 1, giving
\begin{align}
P(k) = \sum_{i=0}^{n}s_{in} = 1, \forall k \in S.
\end{align}

Every element $k\in S$ will appear an integer number of times (since an element either appears or does not). This means, that every probability $s$ will be a rational value, $s\in\mathbb{Q}$ (since it is defined as the total occurrences of $k$ over the total possible occurrences of any element). Therefore, we can define $s^*_{ik}\in \mathbb{N}$ such that $s^*_{ik} = s_{ik}N_k$, where $N_k\in\mathbb{N}$. Given that $N_k$ is the least common denominator (LCD) when summing all values of $s_{ik}$ to 1, this value of $N_k$ would then be the minimum possible number of total elements of $k$, denoted $T_k$, in the initial sequence which produced this matrix. This gives
\begin{align}
\min (T_k) = \sum_{i=0}^{n}s^*_{ik} = N_k, \forall k \in S.
\end{align}
Note that the $\emptyset$ element is a special case here. It is defined as the start or end of a sequence and therefore there will always be $n$ of them from the reference of following an element, and $n$ from the reference of preceding an element.

This is more of an observational conclusion than a formal theorem. Notice in equation \ref{eqn:Example-prob-matrix-1}, for the first column, we have $\frac{2}{3}$ and $\frac{1}{3}$ as the probabilities. When summed to 1, the LCD of these is 3. This correctly matches the value of the number of $A$ elements (which corresponds to the first column) in the corresponding sequence of $S$ which was used. However, if the initial sequence had 6 elements, and the probabilities were thus $\frac{4}{6} = \frac{2}{3}$ and $\frac{2}{6} = \frac{1}{3}$ (respectively), then the value would have been incorrect because the fractions may have been reduced. This is the reason I say it is $\min (T_k)$ rather than $T_k$. Given that the fractions for the probabilities are left in an non-reduced form, or are irreducible to begin with, this would be $T_k$. But for general purposes, the probabilities are likely always reduced to simplest form and thus we can only determine a minimum value of $T_k$. It will be true that the total number is some multiple of $T_k$, however, which can be valuable information.

There may be some interesting insights to gain from the rows of the probability matrix as well. Given the existence of $N_k$, which has already been established, the sum of the row $R$ for some element $k$ can be written as
\begin{align}
	R(k) = \sum_{i=0}^{n} s_{ki} = \sum_{i=0}^{n} \frac{s^*_{ki}}{N_i} = \frac{s^*_{k0}}{N_0} + \frac{s^*_{k1}}{N_1}\frac{s^*_{k2}}{N_2} + \cdots + \frac{s^*_{kn}}{N_n}.
\end{align}
If $N_i$ is an unreduced LCD for the values of its row, then these coefficients give the total number of elements $k$ in the set of sequences
\begin{align}
	T_k = \sum_{i=0}^{n} s^*_{ki}. 
\end{align}
Again, this is more of an observational conclusion than a formal theorem. But this can be seen in the following way. Each element, when appearing in a sequence, will add some minimum amount of probability $p_{min}$ to a corresponding position in the matrix. this amount will always be equal to $1/T_k$. If the elements appear after another element more than once, say, $s^*_{ki}$ times, then this coefficient increments, giving $s^*_{ki}/T_k$. For unreduced fractional forms of each probability, $T_k = N_k$, so $s^*_{ki}/N_k$ is just a count of how many elements there are. Thus, by summing these counters for each case (an element proceeding another element), we have the total number of elements in a set of sequences.



\unfinished




















\subsection{Generating A New Sequence}

One primary use for generating Markov models is so that they can be used to generate a sequence adhering to a similar pattern of the system that the model was generated from. This is useful in many application. One trivial example, which I've used this for in the past, is generating new fantasy-style (Dwarven, Elvish, etc) names based on lists of pre-existing names. In this example case, this type of model served as the perfect tool for generating new and unique names. The newly generated names followed similar stylistic and phonetic patterns of the names from the original list, which gave for a very realistic set of unique names.

When generating a new sequence based on an existing model, a few considerations are needed. The size of the generated sequence needs considered. This can either be fixed (for instance in the case of a set of fixed-sized sequences), randomized (based on some criteria). Typically, I have found that creating a randomized value between the minimum sequence size and maximum sequence size is usually most representative of the data (with some normalization factors for more confined results). 

The procedure for calculating subsequent sequence elements can also be done in a few ways. First, the simplest method is to simply use the exact probabilities from the model, and generate a random subsequent element based on the model weights. There are a few cases which could arise where this type of generation would yield undesirable sequences. First, if you rely solely on the generated probabilities, you will never be able to get results outside of those. For example, for the sequence found in \ref{example-first-order-markov-sequence-small}, you will never get an $A$ followed by a $C$ because it does not ever exist. Alternately, you may have a rather large dataset with a few outliers. For example, a large list of words may contain something like "syzygy". The sequences generated from this could then contain words with "yzy", which may not be desirable for any other sequences but this one. In cases like this, some of the smaller probabilities can be ignored (which usually works nicely for larger data sets).

With the above optional conditions in mind, the steps for generating a sequence follow:
\begin{enumerate}\itemsep0em
	\item Determine the first element of the sequence.
	\item Use the probability matrix of the first element, with appropriate conditional changes, to generate the second element. This probability is based on the elements following the previous element.
	\item Repeat the previous step until the desired sequence size is reached... 
\end{enumerate}

The transition from one element $E_1$ to another element $E_2$ is denoted via $E_1\rightarrow E_2$.  Consider the sequences given in \ref{markov-model-larger-example-sequences}, whose probability matrix is given in \ref{markov-model-larger-example-probability-matrix}. In this example, to begin a sequence is to begin with the element $\emptyset$ (representing the start of a sequence). The only element which ever follows this one is $A$ with $100\%$ probability. So sticking to the given probabilities, a new sequence would begin as $\emptyset\rightarrow A$. Continuing this pattern, the elements which can follow $A$ are $A$, $B$, or $C$. Suppose we randomly choose $C$, which gives the new sequence $\emptyset\rightarrow A\rightarrow C$. Continuing this patter could lead to a final sequence (as an example) of $ACEED$. Thus, a new, perfectly valid sequence is generated using the probability matrix. 

Repeating this pattern can give any number of elements, though some issues may arise which require some edge-case corrections to complete the sequence. For example, suppose we have a sequence starting with $ABCD$. Since $D\rightarrow C$ is perfectly valid, this sequence could continue as $ABCDC$. However, this sequence is already five elements long, and note that $C\rightarrow\emptyset$ is not a valid transition. Therefore, if we strictly held to the rules of this probability matrix, $ABCDC$ would be troublesome in that it is either non-valid or would have to break the rules allowing $C\rightarrow\emptyset$. With these models, certain rules like this often need \textit{broken} in order to provide a path to completion for some sequences (though this is entirely a preferential choice depending on the use).























\subsection{Higher-Order Markov Model}

A higher order Markov model follows similar principals as a first-order Markov model. The difference is how many elements in sequence are tracked and considered when determining another. Consider the same sequence found in \ref{example-first-order-markov-sequence-small}. Suppose the order of this model is $O$ (that is a $O$-order Markov model). This would mean that instead of calculating the probability that an element follows another, the probability matrix would instead contain the probability of an element following any sequence of elements from 1 to $O$. For example, suppose we want $O=2$, giving a second order Markov model. 

There are actually multiple ways to construct the probability matrix depending on the intended purpose of the model. For the sequence in \ref{example-first-order-markov-sequence-small}, one possible matrix can be constructed from the possible combinations of any sequence of 1 \textit{or} 2 elements, which are the same elements in the first-order Markov model with the union of the additional probabilities of every sequence of 2 elements. The sequence of two elements within $S$ in this case are $\emptyset A$ (twice), $\emptyset B$ (once), $AB$ (twice), $BC$ (once), $BD$ (once), $BA$ (once), $AD$ (once), $C\emptyset$ (once) and $D\emptyset$ (twice). The probability for a second order is denoted by $P^2$. This can be written as
\begin{align}
P_{\emptyset A} &= \{A:0, B:2/2, C:0, D:0, \emptyset:0 \} \\
P_{\emptyset B} &= \{A:1/1, B:0, C:0, D:0, \emptyset:0 \} \\
P_{AB} &= \{A:0, B:0, C:1/2, D:1/2, \emptyset:0 \} \\
P_{BA} &= \{A:0, B:0, C:0, D:1/1, \emptyset:0 \} \\
P_{AD} &= \{A:0, B:0, C:0, D:0, \emptyset:1/1 \} \\
P_{BC} &= \{A:0, B:0, C:0, D:0, \emptyset:1/1 \} \\
P_{BD} &= \{A:0, B:0, C:0, D:0, \emptyset:1/1 \} \\
\end{align}
The complete matrix $P^2$ could be constructed by taking this and the union of the matrix given in \ref{eqn:Example-prob-matrix-1}. It could also be constructed into its own matrix without any consideration for single element transitions. A third approach is to construct this matrix with the union of only the single element transitions which don't overlap with the two-element sequences. For example, the elements of $\emptyset$ (which denotes the start of a sequence), would not be a two-element sequence, but rather just a single element, which is then followed by either $A$ or $B$. This would be $P_\emptyset$, which could be included in the above list. In this example, there are no other cases, but other examples would have more. 

Taking the union of all individual element sequences with the multi-element sequences could be a preferred way of constructing these probability matrices for a more complete model because they account for all the multi-element sequences, and then also provide individual element sequences if those do not exist. This is particularly useful when generating new sequences. If you are generating a new sequence, it could be common that you come across a sequence of elements which do not have an existing element proceeding it. In this case, you could shorten the element by one, and check against that sequence to determine the next.


